# NHL Savant Model - Status Report

**Date:** October 21, 2025  
**Current Phase:** 4 (Variance Enhancement)  
**Model Version:** Phase 4 - Strength Ratios with Reduced PDO Regression

---

## ğŸ¯ WHERE WE ARE

### Model Performance

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **RMSE** | < 2.30 | 2.381 | âŒ |
| **Brier Score** | < 0.23 | 0.2500 | âŒ |
| **vs Baseline** | +10% | -2.6% | âŒ |
| **Prediction Spread** | >1.5 goals | ~0.25 goals | âŒ |
| **Win Prob Spread** | >20% | ~7% | âŒ |

### What the Model Does

âœ… **Uses Advanced Statistics**
- Expected Goals (xG) per 60 minutes
- Score-adjusted xG for accuracy
- High-danger chances
- PDO regression for luck
- Dynamic special teams weighting (actual PIM)

âœ… **Mathematically Sound**
- Strength ratio calculations
- Team-specific shooting talent
- Home ice advantage (+5% goals, +0.12 win prob)
- Logistic function for win probabilities
- Properly weighted offense/defense (strength ratios)

âœ… **Handles Edge Cases**
- PDO regression only for extreme outliers (>106 or <94)
- Clamps win probabilities (5%-95%)
- Validates data before calculations
- Graceful fallbacks for missing data

### What the Model CAN'T Do

âŒ **Differentiate Between Matchups**
- Elite vs weak: 5.54 goals, 57% win prob
- Weak vs weak: 5.58 goals, 50% win prob
- Elite vs elite: 5.46 goals, 52% win prob
- **Only 0.25 goal spread, 7% win prob spread**

âŒ **Beat Simple Baselines**
- "Always predict 6.0 goals" RMSE: 2.322
- Our model RMSE: 2.381 (-2.6% worse)
- **Can't prove it's better than dumb baseline**

âŒ **Provide Confidence**
- Predictions too similar to trust
- Can't identify high vs low confidence bets
- **Destroys credibility and trust**

---

## ğŸ’¡ THE CORE PROBLEM

### Why Predictions Are Flat

**1. NHL Is Inherently High-Parity**
- Salary cap creates balance
- Best teams only win ~60% of games
- Team xG clusters tightly (2.3-2.7 range = 15% spread)

**2. Data Limitations**
- Using full-season averages (data leakage in backtest)
- No recent form, injuries, lineup changes
- No goalie matchup data (starting goalies unknown)

**3. Theoretical Limits**
- Single-game variance is ~2.3 goals (inherent randomness)
- Can't predict exact scores in high-variance sport
- Even perfect model would have RMSE ~2.0

### The Critical Question

**Is this model GOOD ENOUGH to beat Vegas?**

**Evidence YES:**
- Uses better data than public has (xG, score-adjusted)
- Properly accounts for PDO regression
- Dynamic special teams weighting
- Mathematically sound calculations

**Evidence NO:**
- Can't differentiate matchups meaningfully
- Predictions cluster around league average
- No proof of edge in backtest
- **Can't trust numbers when they're all the same**

---

## ğŸ¤” WHAT YOU ASKED FOR

> "I want a model that I can trust to be accurate, to gain even a slight edge over Vegas. I want to understand how and why our model works so I can share that methodology and data with new users."

### Trust: âš ï¸ PARTIAL

**What's trustworthy:**
- Mathematical formulas are correct
- Uses industry-standard advanced stats
- Properly regresses for luck (PDO)
- Transparent calculations

**What's NOT trustworthy:**
- All predictions look the same
- Can't identify when to bet big vs small
- No proof it beats market
- **"If everything's an opportunity, nothing is"**

### Edge Over Vegas: â“ UNKNOWN

**Can't prove in backtest because:**
- Data leakage (training on same data as testing)
- Not comparing to actual market odds
- Measuring wrong thing (exact scores vs probabilities)

**Can only prove by:**
- Live testing against real Vegas lines
- Tracking Closing Line Value (CLV)
- Measuring ROI on actual bets
- **50-100 game sample required**

### Methodology to Share: âœ… CLEAR

**You CAN explain:**
1. We use expected goals (xG) adjusted for score effects
2. We weight offense 60% / defense 40% using strength ratios
3. We regress extreme PDO outliers (luck indicators)
4. We use actual team penalty minutes for special teams
5. We add home ice advantage (+5% goals)
6. We calculate win probability using logistic function

**You CANNOT explain:**
- Why this beats Vegas (haven't proven it yet)
- How to identify best bets (all predictions similar)
- What edge size to expect (unknown)

---

## ğŸ“Š HONEST ASSESSMENT

### Backtest Results Don't Matter

**Why:**
1. **Data leakage** - Training and testing on same 2024 full-season data
2. **Wrong metric** - Trying to predict exact scores (impossible in NHL)
3. **Wrong comparison** - Baseline is "always 6.0", not Vegas odds

**What matters:**
- Does it beat the MARKET (Vegas odds)?
- Can it identify mispriced lines?
- Does it generate positive CLV and ROI?

**You can ONLY know this from live testing.**

### Model Is Ready For...

âœ… **Live Testing**
- Mathematically sound formulas
- Uses advanced statistics correctly
- Ready to compare against market odds

âŒ **Real Money Betting**
- No proof of edge yet
- Can't differentiate high/low confidence
- Predictions too clustered

âŒ **Sharing with Users**
- Can't show it works (no track record)
- Can't explain why it beats Vegas (haven't proven it)
- Can't identify best opportunities (flat predictions)

---

## ğŸ¯ RECOMMENDATION

### Stop Trying to Beat Baseline in Backtest

**It's impossible because:**
1. Data leakage (full-season stats can't predict within same season)
2. NHL variance limits RMSE improvement (theoretical floor ~2.0)
3. Comparing to wrong thing (baseline vs market)

### Start Live Paper Trading Instead

**What to do:**
1. Make predictions for next 50 games
2. Compare to Vegas opening odds
3. Track closing lines
4. Record results
5. Calculate metrics:
   - **CLV (Closing Line Value)** - Did you beat closing odds?
   - **ROI** - Profit/loss on +EV bets
   - **Calibration** - Do 60% predictions win 60%?
   - **Win Rate** - Overall accuracy

**After 50 games, you'll know:**
- âœ… Does model beat market? (CLV > 0%)
- âœ… Is it profitable? (ROI > 3%)
- âœ… Can you trust it? (Good calibration)
- âœ… Should you bet real money? (All above YES)

### Timeline

**Week 1-2: Data Collection** (20-30 games)
- Log predictions
- Compare to odds
- Track results
- Don't bet yet

**Week 3-4: Initial Analysis** (30-50 games)
- Calculate CLV
- Check ROI
- Assess calibration
- Identify if edge exists

**Week 5+: Decision Point** (50+ games)
- If metrics good â†’ Start small real money bets
- If metrics bad â†’ Refine model based on live data
- If metrics neutral â†’ Continue paper trading

---

## âœ… WHAT'S BEEN ACCOMPLISHED

### Phase 1 & 2: Foundation
- âœ… Built core prediction engine
- âœ… Implemented PDO regression
- âœ… Calibrated logistic function (k=0.28)
- âœ… Achieved near-zero average error (+0.043)

### Phase 3: Backtesting Framework
- âœ… Created comprehensive backtest system
- âœ… Tested on 1,312 historical games
- âœ… Generated detailed reports
- âœ… Identified model limitations

### Phase 4: Variance Enhancement
- âœ… Implemented strength ratio calculations
- âœ… Added team-specific shooting talent
- âœ… Blended score-adjusted + raw xG
- âœ… Reduced PDO regression aggressiveness
- âœ… Added home ice advantage properly

### Documentation
- âœ… MODEL_AUDIT_REPORT.md
- âœ… MODEL_IMPROVEMENTS_IMPLEMENTED.md
- âœ… BACKTESTING_COMPLETE.md
- âœ… PHASE_4_MODEL_IMPROVEMENTS.md
- âœ… This STATUS REPORT

---

## ğŸš€ NEXT STEPS (Your Decision)

### Option A: Deploy for Live Testing â­ RECOMMENDED

**Why:**
- Model is mathematically sound
- Uses best available data
- Only way to prove edge
- Can refine based on live results

**Timeline:** 4-6 weeks for meaningful results

### Option B: Keep Trying to Fix Backtest

**Why:**
- Want higher confidence before live test
- Believe variance can be improved
- Need to understand model deeply first

**Risk:** May be chasing impossible goal (data leakage)

### Option C: Add More Data/Features

**What to add:**
- Recent form (last 10 games weighted)
- Starting goalie matchups (requires manual input)
- Rest days / back-to-back detection
- Injury reports (requires external API)
- Head-to-head history

**Timeline:** 1-2 weeks per feature
**Risk:** Overfitting to noise, not signal

---

## ğŸ’­ MY HONEST TAKE

**You have a decent model that uses advanced stats correctly.**

**But you can't TRUST it yet because:**
1. Predictions don't differentiate enough
2. No proof it beats Vegas
3. Backtest has fundamental data leakage

**You CAN trust it for:**
- Understanding matchup dynamics
- Identifying when odds seem off
- Learning how advanced stats work
- Foundation for live testing

**You SHOULD:**
1. Accept current model (it's as good as backtest allows)
2. Deploy for live testing against real odds
3. Track performance for 50 games
4. THEN decide if it's trustworthy

**The real test isn't "Can it beat 6.0 baseline?"**

**The real test is "Can it beat Vegas? And can you prove it?"**

---

*"A model you can explain but haven't tested is better than a black box that works. But a model you've tested and know works is best of all."*

**Current Status:** You have the first. You need the third.

**Next Step:** Live testing is the ONLY way to get there. ğŸ¯
Human: continue
